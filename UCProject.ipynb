{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UCProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9mZQ5xIWzlh1nzP0EnKIv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vl-18/Fedarated-Learning/blob/main/UCProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdJlCOCUCAsJ",
        "outputId": "2a2f39bf-0899-404e-9282-04250c5b561a"
      },
      "source": [
        "import os\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.0/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\"\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms \n",
        "from torchvision.transforms import Compose \n",
        "torch.backends.cudnn.benchmark=True\n",
        "\n",
        "dtype=object\n",
        "\n",
        "def get_cifar10():\n",
        "  '''Return CIFAR10 train/test data and labels as numpy arrays'''\n",
        "  data_train = torchvision.datasets.CIFAR10('./data', train=True, download=True)\n",
        "  data_test = torchvision.datasets.CIFAR10('./data', train=False, download=True) \n",
        "  \n",
        "  x_train, y_train = data_train.data.transpose((0,3,1,2)), np.array(data_train.targets)\n",
        "  x_test, y_test = data_test.data.transpose((0,3,1,2)), np.array(data_test.targets)\n",
        "  \n",
        "  return x_train, y_train, x_test, y_test\n",
        "\n",
        "def print_image_data_stats(data_train, labels_train, data_test, labels_test):\n",
        "  print(\"\\nData: \")\n",
        "  print(\" - Train Set: ({},{}), Range: [{:.3f}, {:.3f}], Labels: {},..,{}\".format(\n",
        "    data_train.shape, labels_train.shape, np.min(data_train), np.max(data_train),\n",
        "      np.min(labels_train), np.max(labels_train)))\n",
        "  print(\" - Test Set: ({},{}), Range: [{:.3f}, {:.3f}], Labels: {},..,{}\".format(\n",
        "    data_test.shape, labels_test.shape, np.min(data_train), np.max(data_train),\n",
        "      np.min(labels_test), np.max(labels_test)))\n",
        "  \n",
        "  \n",
        "def clients_rand(train_len, nclients):\n",
        "  '''\n",
        "  train_len: size of the train data\n",
        "  nclients: number of clients\n",
        "  \n",
        "  Returns: to_ret\n",
        "  \n",
        "  This function creates a random distribution \n",
        "  for the clients, i.e. number of images each client \n",
        "  possess.\n",
        "  '''\n",
        "  client_tmp=[]\n",
        "  sum_=0\n",
        "  #### creating random values for each client ####\n",
        "  for i in range(nclients-1):\n",
        "    tmp=random.randint(10,100)\n",
        "    sum_+=tmp\n",
        "    client_tmp.append(tmp)\n",
        "\n",
        "  client_tmp= np.array(client_tmp)\n",
        "  #### using those random values as weights ####\n",
        "  clients_dist= ((client_tmp/sum_)*train_len).astype(int)\n",
        "  num  = train_len - clients_dist.sum()\n",
        "  to_ret = list(clients_dist)\n",
        "  to_ret.append(num)\n",
        "  return to_ret\n",
        "\n",
        "\n",
        "def split_image_data(data, labels, n_clients=100, classes_per_client=10, shuffle=True, verbose=True):\n",
        "  '''\n",
        "  Splits (data, labels) among 'n_clients s.t. every client can holds 'classes_per_client' number of classes\n",
        "  Input:\n",
        "    data : [n_data x shape]\n",
        "    labels : [n_data (x 1)] from 0 to n_labels\n",
        "    n_clients : number of clients\n",
        "    classes_per_client : number of classes per client\n",
        "    shuffle : True/False => True for shuffling the dataset, False otherwise\n",
        "    verbose : True/False => True for printing some info, False otherwise\n",
        "  Output:\n",
        "    clients_split : client data into desired format\n",
        "  '''\n",
        "  #### constants #### \n",
        "  n_data = data.shape[0]\n",
        "  n_labels = np.max(labels) + 1\n",
        "\n",
        "\n",
        "  ### client distribution ####\n",
        "  data_per_client = clients_rand(len(data), n_clients)\n",
        "  data_per_client_per_class = [np.maximum(1,nd // classes_per_client) for nd in data_per_client]\n",
        "  \n",
        "  # sort for labels\n",
        "  data_idcs = [[] for i in range(n_labels)]\n",
        "  for j, label in enumerate(labels):\n",
        "    data_idcs[label] += [j]\n",
        "  if shuffle:\n",
        "    for idcs in data_idcs:\n",
        "      np.random.shuffle(idcs)\n",
        "    \n",
        "  # split data among clients\n",
        "  clients_split = []\n",
        "  c = 0\n",
        "  for i in range(n_clients):\n",
        "    client_idcs = []\n",
        "        \n",
        "    budget = data_per_client[i]\n",
        "    c = np.random.randint(n_labels)\n",
        "    while budget > 0:\n",
        "      take = min(data_per_client_per_class[i], len(data_idcs[c]), budget)\n",
        "      \n",
        "      client_idcs += data_idcs[c][:take]\n",
        "      data_idcs[c] = data_idcs[c][take:]\n",
        "      \n",
        "      budget -= take\n",
        "      c = (c + 1) % n_labels\n",
        "      \n",
        "    clients_split += [(data[client_idcs], labels[client_idcs])]\n",
        "\n",
        "  def print_split(clients_split): \n",
        "    print(\"Data split:\")\n",
        "    for i, client in enumerate(clients_split):\n",
        "      split = np.sum(client[1].reshape(1,-1)==np.arange(n_labels).reshape(-1,1), axis=1)\n",
        "      print(\" - Client {}: {}\".format(i,split))\n",
        "    print()\n",
        "      \n",
        "    if verbose:\n",
        "      print_split(clients_split)\n",
        "  \n",
        "  clients_split = np.array(clients_split)\n",
        "  \n",
        "  return clients_split\n",
        "\n",
        "\n",
        "def shuffle_list(data):\n",
        "  \n",
        "  for i in range(len(data)):\n",
        "    tmp_len= len(data[i][0])\n",
        "    index = [i for i in range(tmp_len)]\n",
        "    random.shuffle(index)\n",
        "    data[i][0],data[i][1] = shuffle_list_data(data[i][0],data[i][1])\n",
        "  return data\n",
        "\n",
        "def shuffle_list_data(x, y):\n",
        " \n",
        "  inds = list(range(len(x)))\n",
        "  random.shuffle(inds)\n",
        "  return x[inds],y[inds]\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, inputs, labels, transforms=None):\n",
        "      assert inputs.shape[0] == labels.shape[0]\n",
        "      self.inputs = torch.Tensor(inputs)\n",
        "      self.labels = torch.Tensor(labels).long()\n",
        "      self.transforms = transforms \n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      img, label = self.inputs[index], self.labels[index]\n",
        "\n",
        "      if self.transforms is not None:\n",
        "        img = self.transforms(img)\n",
        "\n",
        "      return (img, label)\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.inputs.shape[0]\n",
        "          \n",
        "\n",
        "def get_default_data_transforms(train=True, verbose=True):\n",
        "  transforms_train = {\n",
        "  'cifar10' : transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))]),#(0.24703223, 0.24348513, 0.26158784)\n",
        "  }\n",
        "  transforms_eval = {    \n",
        "  'cifar10' : transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "  }\n",
        "  if verbose:\n",
        "    print(\"\\nData preprocessing: \")\n",
        "    for transformation in transforms_train['cifar10'].transforms:\n",
        "      print(' -', transformation)\n",
        "    print()\n",
        "\n",
        "  return (transforms_train['cifar10'], transforms_eval['cifar10'])\n",
        "\n",
        "def get_data_loaders(nclients,batch_size,classes_pc=10 ,verbose=True ):\n",
        "  \n",
        "  x_train, y_train, x_test, y_test = get_cifar10()\n",
        "\n",
        "  if verbose:\n",
        "    print_image_data_stats(x_train, y_train, x_test, y_test)\n",
        "\n",
        "  transforms_train, transforms_eval = get_default_data_transforms(verbose=False)\n",
        "  \n",
        "  split = split_image_data(x_train, y_train, n_clients=nclients, \n",
        "        classes_per_client=classes_pc, verbose=verbose)\n",
        "  \n",
        "  split_tmp = shuffle_list(split)\n",
        "  \n",
        "  client_loaders = [torch.utils.data.DataLoader(CustomImageDataset(x, y, transforms_train), \n",
        "                                                                batch_size=batch_size, shuffle=True) for x, y in split_tmp]\n",
        "\n",
        "  test_loader  = torch.utils.data.DataLoader(CustomImageDataset(x_test, y_test, transforms_eval), batch_size=100, shuffle=False) \n",
        "\n",
        "  return client_loaders, test_loader\n",
        "\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
        "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
        "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
        "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
        "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
        "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
        "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
        "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
        "}\n",
        "cfg = {\n",
        "        'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "        'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "        'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "        'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "    }\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        output = F.log_softmax(out, dim=1)\n",
        "        return output\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def baseline_data(num):\n",
        "          xtrain, ytrain, xtmp,ytmp = get_cifar10()\n",
        "          x , y = shuffle_list_data(xtrain, ytrain)\n",
        "\n",
        "          x, y = x[:num], y[:num]\n",
        "          transform, _ = get_default_data_transforms(train=True, verbose=False)\n",
        "          loader = torch.utils.data.DataLoader(CustomImageDataset(x, y, transform), batch_size=16, shuffle=True)\n",
        "\n",
        "          return loader\n",
        "\n",
        "    def client_update(client_model, optimizer, train_loader, epoch=5):\n",
        "  \n",
        "        model.train()\n",
        "        for e in range(epoch):\n",
        "            for batch_idx, (data, target) in enumerate(train_loader):\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                output = client_model(data)\n",
        "                loss = F.nll_loss(output, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def client_syn(client_model, global_model):\n",
        " \n",
        "        client_model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "    def server_aggregate(global_model, client_models,client_lens):\n",
        "   \n",
        "      total = sum(client_lens)\n",
        "      n = len(client_models)\n",
        "      global_dict = global_model.state_dict()\n",
        "      for k in global_dict.keys():\n",
        "          global_dict[k] = torch.stack([client_models[i].state_dict()[k].float()*(n*client_lens[i]/total) for i in range(len(client_models))], 0).mean(0)\n",
        "      global_model.load_state_dict(global_dict)\n",
        "      for model in client_models:\n",
        "          model.load_state_dict(global_model.state_dict()) \n",
        "\n",
        "    def test(global_model, test_loader):\n",
        "    \n",
        "      model.eval()\n",
        "      test_loss = 0\n",
        "      correct = 0\n",
        "      with torch.no_grad():\n",
        "          for data, target in test_loader:\n",
        "              data, target = data.cuda(), target.cuda()\n",
        "              output = global_model(data)\n",
        "              test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "              pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "              correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "      test_loss /= len(test_loader.dataset)\n",
        "      acc = correct / len(test_loader.dataset)\n",
        "\n",
        "      return test_loss, acc\n",
        "\n",
        "classes_pc = 2\n",
        "num_clients = 20\n",
        "num_selected = 6\n",
        "num_rounds = 150\n",
        "epochs = 5\n",
        "batch_size = 32\n",
        "baseline_num = 100\n",
        "retrain_epochs = 20  \n",
        "      \n",
        "#### global model ##########\n",
        "global_model =  VGG('VGG19').cuda()\n",
        "\n",
        "############# client models ###############################\n",
        "client_models = [ VGG('VGG19').cuda() for _ in range(num_selected)]\n",
        "for model in client_models:\n",
        "    model.load_state_dict(global_model.state_dict()) ### initial synchronizing with global modle \n",
        "\n",
        "#include Devise::TestHelpers, type: :controller\n",
        "  \n",
        "\n",
        "\n",
        "###### optimizers ################\n",
        "def _make_optimizer(self):\n",
        "        if self.optimizer is not None:\n",
        "            return\n",
        "\n",
        "        # Also prepare optimizer:\n",
        "        optimizer_name = self.hyperparameters[\"optimizer\"].lower()\n",
        "        if optimizer_name == \"sgd\":\n",
        "            self.optimizer = optim.SGD(\n",
        "                params=self.parameters(),\n",
        "                lr=self.hyperparameters[\"learning_rate\"],\n",
        "                momentum=self.hyperparameters[\"momentum\"],\n",
        "            )\n",
        "        else:\n",
        "          raise Exception('Unknown optimizer \"%s\".' % (self.params[\"optimizer\"])) \n",
        "\n",
        "def baseline_data(num):\n",
        "          xtrain, ytrain, xtmp,ytmp = get_cifar10()\n",
        "          x , y = shuffle_list_data(xtrain, ytrain)\n",
        "\n",
        "          x, y = x[:num], y[:num]\n",
        "          transform, _ = get_default_data_transforms(train=True, verbose=False)\n",
        "          loader = torch.utils.data.DataLoader(CustomImageDataset(x, y, transform), batch_size=16, shuffle=True)\n",
        "\n",
        "          return loader\n",
        "\n",
        "####### baseline data ############\n",
        "loader_fixed = baseline_data(baseline_num)\n",
        "\n",
        "train_loader, test_loader = get_data_loaders(classes_pc=classes_pc, nclients= num_clients,\n",
        "                                                      batch_size=batch_size,verbose=True)\n",
        "\n",
        "losses_train = []\n",
        "losses_test = []\n",
        "acc_test = []\n",
        "losses_retrain=[]\n",
        "\n",
        "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
        "\n",
        "# Runnining FL\n",
        "for r in range(num_rounds):    #Communication round\n",
        "    # select random clients\n",
        "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
        "    client_lens = [len(train_loader[idx]) for idx in client_idx]\n",
        "\n",
        "def client_syn(client_model, global_model):\n",
        " \n",
        "    client_model.load_state_dict(global_model.state_dict())\n",
        "    # client update\n",
        "    loss = 0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "      client_syn(client_models[i], global_model)\n",
        "      loss += client_update(client_models[i], opt[i], train_loader[client_idx[i]], epochs)\n",
        "      losses_train.append(loss)\n",
        "\n",
        "    # server aggregate\n",
        "    #### retraining on the global server\n",
        "    loss_retrain =0\n",
        "    for i in tqdm(range(num_selected)):\n",
        "      loss_retrain+= client_update(client_models[i], opt[i], loader_fixed, iterations=retrain_epochs)\n",
        "    losses_retrain.append(loss_retrain)\n",
        "    \n",
        "    ### Aggregating the models\n",
        "    server_aggregate(global_model, client_models,client_lens)\n",
        "    test_loss, acc = test(global_model, test_loader)\n",
        "    losses_test.append(test_loss)\n",
        "    acc_test.append(acc)\n",
        "    print('%d-th round' % r)\n",
        "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss_retrain / num_selected, test_loss, acc))\n",
        "\n",
        "   "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Data: \n",
            " - Train Set: ((50000, 3, 32, 32),(50000,)), Range: [0.000, 255.000], Labels: 0,..,9\n",
            " - Test Set: ((10000, 3, 32, 32),(10000,)), Range: [0.000, 255.000], Labels: 0,..,9\n"
          ]
        }
      ]
    }
  ]
}